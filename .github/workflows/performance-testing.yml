name: Performance Testing

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - benchmarks
          - load-tests
          - stress-tests
          - regression-detection

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  # Performance Benchmarks
  benchmarks:
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'benchmarks' || github.event.inputs.test_type == ''

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true

    - name: Cache cargo dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          bc \
          curl \
          jq \
          python3 \
          python3-pip \
          gnuplot \
          imagemagick

    - name: Install Python dependencies for reporting
      run: |
        pip3 install numpy scipy matplotlib seaborn requests

    - name: Run comprehensive benchmarks
      run: |
        echo "Running comprehensive performance benchmarks..."
        cargo bench --bench comprehensive_performance_benchmarks

    - name: Run blockchain performance benchmarks
      run: |
        echo "Running blockchain performance benchmarks..."
        cargo bench --bench blockchain_performance_benchmarks

    - name: Run RDF canonicalization benchmarks
      run: |
        echo "Running RDF canonicalization benchmarks..."
        cargo bench --bench rdf_canonicalization_benchmarks

    - name: Run OWL2 reasoning benchmarks
      run: |
        echo "Running OWL2 reasoning benchmarks..."
        cargo bench --bench owl2_benchmarks

    - name: Process benchmark results
      run: |
        mkdir -p performance-results/benchmarks
        find target/criterion -name "*.json" -exec cp {} performance-results/benchmarks/ \;

    - name: Generate benchmark report
      run: |
        python3 scripts/generate_benchmark_report.py \
          --input performance-results/benchmarks \
          --output performance-results/benchmark_report.html

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: performance-results/
        retention-days: 30

    - name: Comment PR with benchmark results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = './performance-results/benchmark_summary.txt';

          if (fs.existsSync(path)) {
            const summary = fs.readFileSync(path, 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## üìä Performance Benchmark Results\n\n${summary}`
            });
          }

  # Load Testing
  load-tests:
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'load-tests' || github.event.inputs.test_type == ''

    services:
      # Redis for caching (if needed)
      redis:
        image: redis:7
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true

    - name: Cache cargo dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          apache2-utils \
          wrk \
          bc \
          curl \
          jq \
          htop

    - name: Install Node.js for frontend tests
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json

    - name: Install frontend dependencies
      working-directory: ./frontend
      run: npm ci

    - name: Build frontend
      working-directory: ./frontend
      run: npm run build

    - name: Build ProvChain
      run: cargo build --release

    - name: Start ProvChain server
      run: |
        cargo run --release -- web-server --port 8080 &
        sleep 10

    - name: Run load tests
      run: |
        echo "Starting load tests..."
        cargo test --release --test load_tests -- --nocapture

    - name: Run API load tests with wrk
      run: |
        echo "Running API load tests with wrk..."

        # Health check endpoint
        wrk -t12 -c400 -d30s --timeout 10s --latency \
          http://localhost:8080/api/health > performance-results/wrk_health.txt

        # Traceability endpoint
        wrk -t12 -c200 -d60s --timeout 10s --latency \
          -s scripts/post_sparql.lua \
          http://localhost:8080/api/traceability/query > performance-results/wrk_traceability.txt

    - name: Run frontend performance tests
      working-directory: ./frontend
      run: |
        npm run test:performance

    - name: Process load test results
      run: |
        mkdir -p performance-results/load-tests
        python3 scripts/process_load_test_results.py \
          --input performance-results/ \
          --output performance-results/load_test_report.html

    - name: Upload load test results
      uses: actions/upload-artifact@v3
      with:
        name: load-test-results
        path: performance-results/
        retention-days: 30

  # Stress Testing
  stress-tests:
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'stress-tests' || github.event.inputs.test_type == ''

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true

    - name: Cache cargo dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          bc \
          curl \
          jq \
          python3 \
          python3-pip \
          stress-ng

    - name: Install Python dependencies
      run: |
        pip3 install numpy scipy matplotlib seaborn

    - name: Build ProvChain
      run: cargo build --release

    - name: Run stress tests
      run: |
        echo "Running stress tests..."
        cargo test --release --test stress_tests -- --nocapture

    - name: Generate stress test report
      run: |
        mkdir -p performance-results/stress-tests
        python3 scripts/generate_stress_test_report.py \
          --input target/release/deps/ \
          --output performance-results/stress_test_report.html

    - name: Upload stress test results
      uses: actions/upload-artifact@v3
      with:
        name: stress-test-results
        path: performance-results/
        retention-days: 30

  # Frontend Performance Tests
  frontend-performance:
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == ''

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json

    - name: Install frontend dependencies
      working-directory: ./frontend
      run: npm ci

    - name: Run bundle size analysis
      working-directory: ./frontend
      run: |
        npm run build
        npx webpack-bundle-analyzer dist/static/js/*.*.js \
          --mode static --report html --report-file performance-results/bundle-analysis.html

    - name: Run Lighthouse CI
      working-directory: ./frontend
      run: |
        npm install -g @lhci/cli@0.12.x
        npm run build
        npm run start &
        sleep 10

        lhci autorun \
          --config=.lighthouserc.json \
          --upload.target=filesystem \
          --upload.outputDir=../performance-results/lighthouse

    - name: Run frontend performance tests
      working-directory: ./frontend
      run: |
        npm run test:performance -- --coverage
        npm run test:e2e:performance

    - name: Upload frontend performance results
      uses: actions/upload-artifact@v3
      with:
        name: frontend-performance-results
        path: performance-results/
        retention-days: 30

  # Performance Regression Detection
  regression-detection:
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'regression-detection' || github.event.inputs.test_type == ''
    needs: [benchmarks, load-tests]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install Python dependencies
      run: |
        pip3 install numpy scipy matplotlib seaborn requests

    - name: Download previous results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results
        path: previous-results/

    - name: Download current results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results
        path: current-results/

    - name: Run performance regression detection
      run: |
        # Setup baseline if it doesn't exist
        if [ ! -f metrics/performance_baseline.json ]; then
          mkdir -p metrics
          cp previous-results/benchmark_summary.json metrics/performance_baseline.json
        fi

        # Extract current metrics
        cp current-results/benchmark_summary.json metrics/current_metrics.json

        # Run regression detection
        python3 scripts/performance_regression_detector.py \
          --generate-report \
          --generate-charts \
          --output-dir performance-results/regression

    - name: Upload regression results
      uses: actions/upload-artifact@v3
      with:
        name: regression-results
        path: performance-results/
        retention-days: 30

    - name: Performance regression check
      run: |
        python3 scripts/performance_regression_detector.py
        exit_code=$?

        if [ $exit_code -eq 1 ]; then
          echo "‚ùå Critical performance regressions detected!"
          exit 1
        elif [ $exit_code -eq 2 ]; then
          echo "‚ö†Ô∏è Performance regressions detected (no critical issues)"
          exit 0
        else
          echo "‚úÖ No performance regressions detected!"
        fi

  # Performance Report Generation
  generate-report:
    runs-on: ubuntu-latest
    if: always() && (github.event.inputs.test_type == 'all' || github.event.inputs.test_type == '')
    needs: [benchmarks, load-tests, stress-tests, frontend-performance, regression-detection]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install Python dependencies
      run: |
        pip3 install jinja2 markdown numpy scipy matplotlib seaborn

    - name: Download all results
      uses: actions/download-artifact@v3
      with:
        path: all-results/

    - name: Generate comprehensive performance report
      run: |
        python3 scripts/generate_comprehensive_report.py \
          --input-dir all-results/ \
          --output-dir performance-report/ \
          --template templates/performance_report.md

    - name: Upload final report
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-performance-report
        path: performance-report/
        retention-days: 90

    - name: Deploy report to GitHub Pages
      if: github.ref == 'refs/heads/main'
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: performance-report/
        destination_dir: performance-reports

    - name: Comment PR with performance summary
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const summaryPath = './performance-report/performance_summary.txt';

          if (fs.existsSync(summaryPath)) {
            const summary = fs.readFileSync(summaryPath, 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## üöÄ Performance Test Results\n\n${summary}\n\n[View Full Report](https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/performance-reports/)`
            });
          }

  # Performance Alerts
  performance-alerts:
    runs-on: ubuntu-latest
    if: failure() && github.event_name == 'push'
    needs: [benchmarks, load-tests, stress-tests, regression-detection]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Send Slack notification on failure
      if: env.SLACK_WEBHOOK_URL != ''
      run: |
        curl -X POST -H 'Content-type: application/json' \
          --data '{"text":"üö® Performance test failures detected in commit ${{ github.sha }} on branch ${{ github.ref_name }}\n\nView details: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"}' \
          ${{ secrets.SLACK_WEBHOOK_URL }}

    - name: Send email notification on failure
      if: env.ALERT_EMAIL != ''
      uses: dawidd6/action-send-mail@v3
      with:
        server_address: smtp.gmail.com
        server_port: 587
        username: ${{ secrets.EMAIL_USERNAME }}
        password: ${{ secrets.EMAIL_PASSWORD }}
        subject: "ProvChain Performance Test Failure - ${{ github.ref_name }}"
        body: |
          Performance tests failed in commit ${{ github.sha }} on branch ${{ github.ref_name }}.

          Commit: ${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }}
          Actions: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

          Please review the test results and address any performance issues.
        to: ${{ secrets.ALERT_EMAIL }}
        from: ${{ secrets.EMAIL_FROM }}